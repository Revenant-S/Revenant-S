---
title: 2020-12-01-爬虫基础 
date: 2020-12-01 10:05:26
categories:
- 2020-12
tags:
- Python
- 爬虫
---



# Python爬虫

## 爬虫用途

****

**网络爬虫**：按照一定的规则，自动地爬取互联网信息的程序。

<!-- more -->

## 应用方向

****

###  定制搜索引擎

**学习爬虫，可以私人定制一个搜索引擎，并且可以对搜索引擎的数据工作原理进行更深层次的理解**

### seo优化

学习爬虫，可以更深层次理解搜索引擎爬虫工作原理，从而**可以更好地进行搜索引擎优化**

### 数据分析

****

**大数据时代，要进行数据分析，首先要有数据源，而学习爬虫，可以让我们获取更多的数据，并且这些数据源可以按我们的目的进行采集，去掉很多无关数据。**		

### 找工作

----

**还是要恰饭的嘛。**

## 为什么要用Python爬虫

****

1. PHP：天生不适合
2. Java：生态圈完善，但语言本身较笨重，毕竟人生苦短......
3. C/C++：运行效率没得说，大拇指，但学起来也太难了叭。
4. Python：代码简洁优美，开发效率高。谁叫我这么菜，只想学简单的。

### 一个python爬虫

使用python爬取百度网址

```python
import requests  # 导入爬虫requests库

url = 'http://www.baidu.com'
response = requests.get(url=url)  # 向指定url地址发送请求
print(response)  # 返回状态码，200表示请求成功
print(response.text)  # 返回百度页面的html
```

# 爬虫

## 爬虫分类

****

### 通用爬虫

通用爬虫是搜索引擎抓取系统（百度、Google、Sogou等）一个重要组成部分。主要是将互联网上的网页下载到本地，形成一个互联网内容的镜像备份，为搜索引擎提供搜索支撑。

**搜索引擎工作原理：**

![img](https://cdn.jsdelivr.net/gh/Monster1314/blog/img/search.png)

- **第一步： 抓取网页**

  搜索引擎去网站上抓取数据

- **第二步：数据存储**

  搜索引擎通过爬虫得到的网页，将数据存入原始页面数据库（文档库）。其中的页面数据与用户浏览器得到的HTML是完全一样的。

- **第三步：提供检索服务，网站排名**

  搜索引擎将爬虫抓取回来的页面，进行各种步骤的预处理：中文分词，消除噪音，索引处理等

**搜索引擎局限性**：

1. 搜索引擎抓取的是整个网页，不是具体详细的信息；
2. 搜索引擎无法提供针对具体某个客户需求的搜索结果。

### 聚焦爬虫

聚焦爬虫，是“面向特定主题需求”的一种网络爬虫程序，它与通用搜索引擎爬虫的区别在于：**聚焦爬虫在实施网页抓取时会对内容进行处理筛选，尽量保证只抓取与需求相关的网页数据。**

我们学当然要学聚焦爬虫 =v= 。

###  Robots协议

Robots协议也称爬虫协议、机器人协议等，全称呢个“网络爬虫排除标准”（Robots Exclusion Protocol）,网站通过Robot协议告诉搜索引擎哪些页面可以抓取，哪些页面不能抓取，如：

淘宝：<https://www.taobao.com/robots.txt>

百度：<https://www.baidu.com/robots.txt>

# 请求与相应

1. 当我们在浏览器输入https://www.baidu.com时，浏览器会发送一个Request请求去获取https://baidu.com的html文件，服务器返回Response对象给浏览器。
2. 浏览器分析Response中的HTML，发现其中引用了很多文件，如Images文件、CSS文件。浏览器会自动再次发送请求去获取图片等。
3. 当所有文件都下载成功后，网页会根据HTML语法结构，完整显示出来。

# Chrome 开发者工具

一般爬虫用的最多的是**Chrome开发者工具**，在谷歌浏览器中右击，左击“检查”，或者按快捷键F12即可打开开发者工具。

## 元素面板（Elements）

****

![img](https://cdn.jsdelivr.net/gh/Monster1314/blog/img/elements.png)

打开谷歌开发者工具，点击elements，在这里可以看到网页的html，同时后续css/xpath/re都是从这里获取资源定位。

选页面某一节点右击也可复制xpath路径、css路径等。

## 控制台面板（Console）

****

![img](https://cdn.jsdelivr.net/gh/Monster1314/blog/img/console.png)

控制台面板是用于显示JS和DOM对象信息的单独窗口。js解密会用console调试运行js代码。

## 资源面板（Source）

----

![img](https://cdn.jsdelivr.net/gh/Monster1314/blog/img/source.png)

左侧可以看到源文件以树结构进行展示。

中间可以用来调试js代码。

右侧是断点调试功能区。

##  网络面板（Network）

****

![img](https://cdn.jsdelivr.net/gh/Monster1314/blog/img/network.png)

网络（Network）面板记录页面上每个网络操作的相关信息，包括详细的耗时数据、HTTP标头和Cookie等。

###  工具栏

![img](https://cdn.jsdelivr.net/gh/Monster1314/blog/img/tool.png)

从左到右：

**Stop recording network log**

默认情况下，只要开发者工具在开启状态，就会记录所有网络请求。红色表示开始，灰色表示关闭。

**Clear**

清空所有数据

**Filter**

数据包过滤，红色打开，蓝色关闭。经常使用它来过滤一些HTTP请求，如Ajax异步请求、图片、视频等。

**Search**

搜索框，只要在ALL里出现过的内容，就可以被直接搜索到。常用于数据检索与JS解密。

**Preserve log**

保留日志。当分析多个页面跳转的内容时，一定要勾上，不然页面跳转会把历史清空。

**Disable cache**

清空Javascript、css文件缓存，获取最新的。

**Hide data URLs**

用于是否隐藏dataurl。什么是dataurl？传统的通常img标记的src属性指定了一个远程服务器的资源，浏览器针对每个外部资源需要向服务器发送一次拉去资源请求。而Data URL技术是图片数据以base64字符串格式嵌入到了页面中，和HTML融为一体。

###  Requests详情

**请求头**

**Headers**是显示HTTP请求的Headers，我们通过这个能看到请求的方式，以及携带的请求参数等。

- General

- Response Headers

- Request Headers

  

**预览**

Preview是请求结果的预览。

**响应体**

Response是请求返回的结果，一般是整个网站的源代码。如果该请求是异步请求，返回的结果一般是json文本数据。

此数据与浏览器展示的网页可能不一致，因为浏览器是动态加载的。